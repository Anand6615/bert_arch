{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIRST LAYER**\n",
    "\n",
    "1. Introduction to BERT and the First Layer of Embeddings\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer model designed to understand the context of words in a sentence bidirectionally. It processes the entire sentence at once (both left-to-right and right-to-left) to capture rich context.\n",
    "\n",
    "The first layer in BERT is where the model starts processing input text. Each word or token in the input is transformed into an embedding that contains information about its meaning and context within the sentence. BERT uses three types of embeddings:\n",
    "\n",
    "Token Embeddings: Represent individual words or tokens.\n",
    "Position Embeddings: Encode the position of each token in the sentence.\n",
    "Segment Embeddings: Indicate which sentence the token belongs to (useful in tasks involving sentence pairs).\n",
    "\n",
    "\n",
    "2. How BERT Embeddings Work\n",
    "Each token in the input sentence is converted into an embedding vector in three steps:\n",
    "\n",
    "Tokenization: The input sentence is split into smaller units called \"tokens\" using BERT's tokenizer.\n",
    "Embedding Layer: The tokens are then converted into three types of embeddings:\n",
    "Token Embeddings: The meaning of the token itself.\n",
    "Position Embeddings: The position of the token in the sentence.\n",
    "Segment Embeddings: Which sentence the token belongs to.\n",
    "\n",
    "\n",
    "3. The Components of BERT's Embedding Layer (First Layer)\n",
    "Token Embeddings\n",
    "What it is: Each token in the input sentence is represented as a high-dimensional vector (usually 768 dimensions for bert-base-uncased).\n",
    "How it's obtained: The tokenizer converts each word or subword into a token. Each token is mapped to a specific embedding in a large lookup table.\n",
    "Example: For the sentence \"BERT is powerful\", the tokenizer might split \"BERT\" into the token [BERT] and \"is\" into [is], each having a corresponding embedding.\n",
    "Position Embeddings\n",
    "What it is: Since transformers don't have a sense of the position of words (as opposed to RNNs), position embeddings are added to provide this information.\n",
    "How it's obtained: Each position in the sentence (i.e., token index) is assigned a unique vector. These position embeddings are learned during pre-training.\n",
    "Example: The position of \"BERT\" would have a different position embedding compared to \"is\" based on their location in the sentence.\n",
    "Segment Embeddings\n",
    "What it is: Segment embeddings are used to differentiate between two sentences in tasks like question answering.\n",
    "How it's obtained: BERT uses 0 for the first sentence and 1 for the second sentence (if any).\n",
    "Example: In tasks like Sentence A: \"What is BERT?\" and Sentence B: \"It is a transformer model,\" the first sentence would have segment embeddings of [0] and the second one would have [1].\n",
    "\n",
    "\n",
    "5. Self-Attention Mechanism in the First Layer\n",
    "The first layer in BERT utilizes the self-attention mechanism to process the embeddings.\n",
    "\n",
    "Self-Attention: Each token attends to every other token in the input to decide how much weight (importance) each token should receive when forming its representation. This happens at every layer of BERT.\n",
    "Why it's important: Self-attention helps BERT capture contextual relationships between words (e.g., \"BERT is a model\" vs \"BERT is a transformer model\").\n",
    "In the First Layer: The first layer’s self-attention learns how each token should interact with others to better represent the sentence structure and context.\n",
    "5. Output of the First Layer\n",
    "Token Representation: After passing through the first layer, each token has a representation that is context-sensitive, i.e., the embedding of \"BERT\" will change depending on surrounding words.\n",
    "Shape of Output: The output shape for each token embedding in the first layer is (batch_size, sequence_length, hidden_size), where:\n",
    "batch_size is the number of sentences in the input batch.\n",
    "sequence_length is the number of tokens in the sentence after tokenization.\n",
    "hidden_size is typically 768 in bert-base.\n",
    "\n",
    "\n",
    "6. Methods Involved in BERT's First Layer\n",
    "Step 1: Tokenization\n",
    "Method: BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "Functionality: Converts raw text into tokens that BERT understands. It uses WordPiece tokenization, breaking words into smaller subwords.\n",
    "Step 2: Embedding Layer\n",
    "Method: The embeddings are created using the embedding lookup table.\n",
    "embedding_tokens: For each token, retrieve its corresponding embedding.\n",
    "embedding_positions: Each token's position gets a position embedding.\n",
    "embedding_segments: For sentence segmentation, retrieve the segment embedding.\n",
    "Step 3: Self-Attention\n",
    "Method: torch.matmul (Matrix multiplication)\n",
    "First, compute the Query, Key, and Value matrices based on the input embeddings.\n",
    "Then compute the attention scores by taking the dot product of the Query and Key matrices, followed by a softmax function to get attention weights.\n",
    "Finally, use these attention weights to compute the output for each token.\n",
    "Step 4: Combine Embeddings\n",
    "Method: The token embeddings, position embeddings, and segment embeddings are combined element-wise.\n",
    "The combined embedding is then passed through the first self-attention layer.\n",
    "\n",
    "7. Key Takeaways for the First Layer\n",
    "Token Embeddings: These embeddings represent the meaning of individual tokens.\n",
    "Position Embeddings: Help provide context for each token's position in the sequence.\n",
    "Segment Embeddings: Used to differentiate between sentences in tasks like sentence pair classification.\n",
    "Self-Attention: The core mechanism that allows BERT to understand the relationships between tokens in the context of the entire sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T12:06:52.701518Z",
     "iopub.status.busy": "2024-11-06T12:06:52.700464Z",
     "iopub.status.idle": "2024-11-06T12:06:53.177578Z",
     "shell.execute_reply": "2024-11-06T12:06:53.176177Z",
     "shell.execute_reply.started": "2024-11-06T12:06:52.701471Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings for first paragraph:\n",
      " tensor([[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "        [ 0.9295,  0.5056,  0.4276,  ...,  0.9511,  0.7785, -0.2679],\n",
      "        [-0.6270, -0.0633, -0.3143,  ...,  0.3427,  0.4636,  0.4594],\n",
      "        ...,\n",
      "        [-0.4845, -0.4881,  0.7400,  ..., -0.3568, -0.2392,  0.1933],\n",
      "        [-0.0812,  0.1353,  0.0899,  ...,  0.0494,  0.7483,  0.5275],\n",
      "        [-0.0908, -0.2099,  0.0628,  ..., -0.7465,  0.4288, -0.2265]])\n",
      "Shape of Token Embeddings: torch.Size([36, 768])\n",
      "\n",
      "Expanded Position Embeddings:\n",
      " tensor([[[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
      "           6.8312e-04,  1.5441e-02],\n",
      "         [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
      "           2.9753e-02, -5.3247e-03],\n",
      "         [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
      "           1.8741e-02, -7.3140e-03],\n",
      "         ...,\n",
      "         [ 6.4262e-03, -2.1805e-02, -1.0427e-03,  ..., -1.7709e-02,\n",
      "           1.8698e-03, -6.5411e-03],\n",
      "         [ 6.3550e-03, -1.0774e-02,  1.1991e-03,  ..., -1.1787e-02,\n",
      "           1.5860e-02, -5.5016e-04],\n",
      "         [ 9.5380e-03, -1.2697e-02, -8.6343e-03,  ..., -1.4068e-02,\n",
      "           2.0607e-02, -4.3995e-03]],\n",
      "\n",
      "        [[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
      "           6.8312e-04,  1.5441e-02],\n",
      "         [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
      "           2.9753e-02, -5.3247e-03],\n",
      "         [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
      "           1.8741e-02, -7.3140e-03],\n",
      "         ...,\n",
      "         [ 6.4262e-03, -2.1805e-02, -1.0427e-03,  ..., -1.7709e-02,\n",
      "           1.8698e-03, -6.5411e-03],\n",
      "         [ 6.3550e-03, -1.0774e-02,  1.1991e-03,  ..., -1.1787e-02,\n",
      "           1.5860e-02, -5.5016e-04],\n",
      "         [ 9.5380e-03, -1.2697e-02, -8.6343e-03,  ..., -1.4068e-02,\n",
      "           2.0607e-02, -4.3995e-03]],\n",
      "\n",
      "        [[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
      "           6.8312e-04,  1.5441e-02],\n",
      "         [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
      "           2.9753e-02, -5.3247e-03],\n",
      "         [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
      "           1.8741e-02, -7.3140e-03],\n",
      "         ...,\n",
      "         [ 6.4262e-03, -2.1805e-02, -1.0427e-03,  ..., -1.7709e-02,\n",
      "           1.8698e-03, -6.5411e-03],\n",
      "         [ 6.3550e-03, -1.0774e-02,  1.1991e-03,  ..., -1.1787e-02,\n",
      "           1.5860e-02, -5.5016e-04],\n",
      "         [ 9.5380e-03, -1.2697e-02, -8.6343e-03,  ..., -1.4068e-02,\n",
      "           2.0607e-02, -4.3995e-03]]], grad_fn=<ExpandBackward0>)\n",
      "Shape of Expanded Position Embeddings: torch.Size([3, 36, 768])\n",
      "\n",
      "Segment Embeddings for first paragraph:\n",
      " tensor([[ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
      "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
      "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
      "        ...,\n",
      "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
      "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
      "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Shape of Segment Embeddings: torch.Size([36, 768])\n",
      "\n",
      "Combined Token + Position + Segment Embeddings for the batch:\n",
      " tensor([[[ 0.1865, -0.3004, -0.3591,  ..., -0.0341,  0.0356,  0.1708],\n",
      "         [ 0.9377,  0.5188,  0.4119,  ...,  0.9734,  0.8049, -0.2818],\n",
      "         [-0.6379, -0.0543, -0.3221,  ...,  0.3509,  0.4790,  0.4434],\n",
      "         ...,\n",
      "         [-0.4777, -0.4989,  0.7427,  ..., -0.3811, -0.2407,  0.1781],\n",
      "         [-0.0744,  0.1355,  0.0948,  ...,  0.0310,  0.7608,  0.5183],\n",
      "         [-0.0808, -0.2116,  0.0579,  ..., -0.7672,  0.4460, -0.2395]],\n",
      "\n",
      "        [[ 0.1865, -0.3004, -0.3591,  ..., -0.0341,  0.0356,  0.1708],\n",
      "         [ 0.6614, -0.9003,  0.5064,  ...,  0.4530, -0.1060, -0.3837],\n",
      "         [-0.1491, -0.7415,  1.0379,  ...,  0.3107,  0.3510, -0.0151],\n",
      "         ...,\n",
      "         [ 0.3727, -0.9790,  0.0089,  ..., -0.4182, -0.2788, -0.0266],\n",
      "         [ 0.3676, -0.7532,  0.0515,  ..., -0.3106, -0.0021,  0.0992],\n",
      "         [ 0.4275, -0.7755, -0.1353,  ..., -0.3444,  0.0922,  0.0195]],\n",
      "\n",
      "        [[ 0.1865, -0.3004, -0.3591,  ..., -0.0341,  0.0356,  0.1708],\n",
      "         [-0.4725,  0.8320, -0.4384,  ..., -0.1192,  1.1328,  0.5290],\n",
      "         [-0.6927,  0.4257, -0.5746,  ..., -0.2009,  0.5026, -0.5247],\n",
      "         ...,\n",
      "         [ 0.3727, -0.9790,  0.0089,  ..., -0.4182, -0.2788, -0.0266],\n",
      "         [ 0.3676, -0.7532,  0.0515,  ..., -0.3106, -0.0021,  0.0992],\n",
      "         [ 0.4275, -0.7755, -0.1353,  ..., -0.3444,  0.0922,  0.0195]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Shape of Combined Embedding Output: torch.Size([3, 36, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Define the paragraphs\n",
    "paragraphs = [\n",
    "    \"BERT is a transformer model developed by Google. It has revolutionized NLP by introducing bidirectional context and allowing pre-training on vast datasets.\",\n",
    "    \"Padding is used to ensure that all inputs in a batch have the same length. This simplifies batch processing in deep learning.\",\n",
    "    \"When dealing with paragraphs, BERT can only process up to 512 tokens at a time. Therefore, long paragraphs may need to be truncated.\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the paragraphs\n",
    "inputs = tokenizer(paragraphs, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.hidden_states  # List of hidden states for each layer, including embeddings\n",
    "\n",
    "# Token Embeddings (retrieved from hidden_states[0] which is the output of the embedding layer)\n",
    "token_embeddings = hidden_states[0]\n",
    "print(\"Token Embeddings for first paragraph:\\n\", token_embeddings[0])\n",
    "print(\"Shape of Token Embeddings:\", token_embeddings[0].shape)\n",
    "print()\n",
    "\n",
    "# Position Embeddings: we need to expand it to match the batch dimension\n",
    "position_embeddings = model.embeddings.position_embeddings.weight[:inputs['input_ids'].shape[1]]\n",
    "position_embeddings = position_embeddings.unsqueeze(0).expand(inputs['input_ids'].size(0), -1, -1)\n",
    "print(\"Expanded Position Embeddings:\\n\", position_embeddings)\n",
    "print(\"Shape of Expanded Position Embeddings:\", position_embeddings.shape)\n",
    "print()\n",
    "\n",
    "# Segment Embeddings\n",
    "segment_ids = inputs['token_type_ids']\n",
    "segment_embeddings = model.embeddings.token_type_embeddings(segment_ids)\n",
    "print(\"Segment Embeddings for first paragraph:\\n\", segment_embeddings[0])\n",
    "print(\"Shape of Segment Embeddings:\", segment_embeddings[0].shape)\n",
    "print()\n",
    "\n",
    "# Combined Embedding Output for each paragraph\n",
    "combined_embedding_output = token_embeddings + position_embeddings + segment_embeddings\n",
    "print(\"Combined Token + Position + Segment Embeddings for the batch:\\n\", combined_embedding_output)\n",
    "print(\"Shape of Combined Embedding Output:\", combined_embedding_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discrepancy in the dimensions is because the token embeddings and segment embeddings are processed in batches (one for each paragraph), resulting in a shape of [batch_size, sequence_length, hidden_size], which is [3, 36, 768] in your case. However, position embeddings are shared across the entire batch and only depend on the sequence length, so they have a shape of [sequence_length, hidden_size], which is [36, 768]\n",
    "\n",
    "position embeddings have a fixed length matching the maximum token length per sentence (e.g., 512), while token and segment embeddings vary by batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SECOND LAYER**\n",
    "\n",
    "1. Self-Attention\n",
    "Self-attention is the key mechanism in BERT, and it computes how each token should attend to every other token in the sequence. In the second layer, BERT applies multi-head self-attention to create contextualized representations of each token, considering the entire input sequence.\n",
    "\n",
    "Q (Query), K (Key), V (Value) Calculation: For each token, three vectors are computed:\n",
    "\n",
    "Query (Q): Represents the token’s perspective (what it looks for in other tokens).\n",
    "Key (K): Represents the information available to other tokens.\n",
    "Value (V): Contains the actual information that will be passed along.\n",
    "These are generated by multiplying the token embeddings with learned weight matrices.\n",
    "\n",
    "Attention Scores Calculation: The attention scores between tokens are calculated by performing a dot product between the query vector of one token and the key vector of all other tokens. This gives an idea of how much attention one token should give to others.\n",
    "\n",
    "\n",
    "Scaled Dot-Product: The result of the dot product is then scaled by the square root of the key dimension to prevent excessively large values that could skew the attention.\n",
    "\n",
    "Softmax: A softmax function is applied to normalize the attention scores, ensuring that they sum to 1, making them interpretable as probabilities.\n",
    "\n",
    "Weighted Sum of Values (V): Finally, the weighted sum of the value vectors is computed, where the weights are the attention scores. This gives a new context-aware representation for each token.\n",
    "\n",
    "2. Multi-Head Attention\n",
    "Instead of using a single set of attention weights, BERT uses multiple heads for self-attention. This allows the model to focus on different parts of the sentence using different perspectives at the same time. After each attention head processes the sequence, the results are concatenated and passed through a linear layer to combine the information.\n",
    "\n",
    "3. Add & Norm\n",
    "Once self-attention has been applied, the result is added to the input (called a residual connection), and layer normalization is performed to stabilize training and help with convergence.\n",
    "\n",
    "Summary of Steps in the Second Layer:\n",
    "Input: Token embeddings (with position and segment embeddings added).\n",
    "Self-Attention: Compute Q, K, V for each token and perform the attention mechanism to get new token representations.\n",
    "Multi-Head Attention: Apply attention in parallel using multiple heads, then concatenate the results.\n",
    "Residual Connection: Add the original token embeddings to the output of the self-attention mechanism.\n",
    "Layer Normalization: Normalize the result to ensure stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:14:46.741499Z",
     "iopub.status.busy": "2024-11-06T12:14:46.741041Z",
     "iopub.status.idle": "2024-11-06T12:14:47.034822Z",
     "shell.execute_reply": "2024-11-06T12:14:47.033440Z",
     "shell.execute_reply.started": "2024-11-06T12:14:46.741456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(paragraphs, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract the token embeddings from the last hidden state\n",
    "token_embeddings = outputs.last_hidden_state\n",
    "# Define linear layers to create Q, K, V matrices\n",
    "hidden_size = token_embeddings.size(-1)\n",
    "W_Q = torch.nn.Linear(hidden_size, hidden_size)\n",
    "W_K = torch.nn.Linear(hidden_size, hidden_size)\n",
    "W_V = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "# Apply Q, K, V transformations to the token embeddings for each token in the batch\n",
    "Q = W_Q(token_embeddings)\n",
    "K = W_K(token_embeddings)\n",
    "V = W_V(token_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:15:52.161761Z",
     "iopub.status.busy": "2024-11-06T12:15:52.160470Z",
     "iopub.status.idle": "2024-11-06T12:15:52.173576Z",
     "shell.execute_reply": "2024-11-06T12:15:52.172442Z",
     "shell.execute_reply.started": "2024-11-06T12:15:52.161671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Calculate the attention scores (dot product of Q and K, scaled by sqrt of hidden size)\n",
    "attention_scores = torch.matmul(Q, K.transpose(-1, -2)) / (hidden_size ** 0.5)\n",
    "# Apply softmax to get attention probabilities\n",
    "attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "# Compute the weighted sum of V vectors based on the attention probabilities\n",
    "attention_output = torch.matmul(attention_probs, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:16:57.165397Z",
     "iopub.status.busy": "2024-11-06T12:16:57.164944Z",
     "iopub.status.idle": "2024-11-06T12:16:57.184396Z",
     "shell.execute_reply": "2024-11-06T12:16:57.183214Z",
     "shell.execute_reply.started": "2024-11-06T12:16:57.165347Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query\n",
      "tensor([[[-0.5497,  0.1242,  0.1225,  ..., -0.2947, -0.6243, -0.4016],\n",
      "         [ 0.0134,  0.3883,  0.3186,  ..., -0.0808, -0.4024, -0.2399],\n",
      "         [ 0.0097, -0.2053,  0.4934,  ..., -0.3806, -0.1386, -0.2752],\n",
      "         ...,\n",
      "         [-0.0224,  0.0101,  0.3008,  ..., -0.2930, -0.1133, -0.2517],\n",
      "         [ 0.3751,  0.0544,  0.1858,  ..., -0.1016, -0.0328, -0.0596],\n",
      "         [-0.1022, -0.3221, -0.3149,  ..., -0.1627, -0.1107, -0.2446]],\n",
      "\n",
      "        [[-0.8369, -0.1108,  0.1554,  ..., -0.0823, -0.3242, -0.2340],\n",
      "         [-0.2528,  0.0139, -0.5795,  ..., -0.1524, -0.2834, -0.8862],\n",
      "         [-0.1286, -0.3708,  0.3216,  ..., -0.3282,  0.0915, -0.3603],\n",
      "         ...,\n",
      "         [-0.1671,  0.0651,  0.0740,  ...,  0.0999, -0.1939, -0.0031],\n",
      "         [-0.0544,  0.3365,  0.2446,  ..., -0.0227, -0.3401,  0.0894],\n",
      "         [-0.1258,  0.0091,  0.0557,  ...,  0.1007, -0.2979,  0.0954]],\n",
      "\n",
      "        [[-0.6766,  0.1456,  0.1801,  ..., -0.1271, -0.2301, -0.2722],\n",
      "         [-0.5991,  0.1168,  0.0158,  ..., -0.5163,  0.1317, -0.3773],\n",
      "         [-0.1644, -0.1526,  0.3795,  ..., -0.0967,  0.0394, -0.5690],\n",
      "         ...,\n",
      "         [-0.0694,  0.2227,  0.2565,  ...,  0.0101,  0.0151, -0.1039],\n",
      "         [-0.2718,  0.2460,  0.2047,  ..., -0.1421, -0.1165,  0.0636],\n",
      "         [-0.1883,  0.2363,  0.2800,  ..., -0.0861, -0.0767,  0.0769]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Length og query is:  36\n",
      "torch.Size([36, 768])\n",
      "\n",
      "Value\n",
      "tensor([[[ 0.4511,  0.4573,  0.1196,  ...,  0.3185, -0.3582, -0.1670],\n",
      "         [ 0.4139, -0.1753, -0.2867,  ...,  0.2507,  0.2148,  0.4523],\n",
      "         [ 0.1097, -0.0956,  0.2387,  ...,  0.0541, -0.1471,  0.4424],\n",
      "         ...,\n",
      "         [ 0.2669, -0.2019,  0.0909,  ...,  0.6827, -0.1095,  0.4583],\n",
      "         [ 0.2485,  0.4560, -0.0942,  ...,  0.7366,  0.4104,  0.0443],\n",
      "         [ 0.4067, -0.2415, -0.3562,  ...,  0.3730,  0.3039,  0.3825]],\n",
      "\n",
      "        [[ 0.4025,  0.2537,  0.0011,  ...,  0.1403, -0.6010, -0.0182],\n",
      "         [ 0.2648, -0.2396,  0.1394,  ...,  0.6940, -0.0093,  0.2692],\n",
      "         [ 0.1198, -0.4762,  0.4544,  ...,  0.4361, -0.2832,  0.3430],\n",
      "         ...,\n",
      "         [-0.0400, -0.0711,  0.0883,  ...,  0.4583, -0.1651,  0.1205],\n",
      "         [ 0.1097, -0.0280,  0.1761,  ...,  0.4153, -0.3103,  0.1394],\n",
      "         [-0.0994,  0.0418,  0.1125,  ...,  0.3854, -0.2016,  0.1722]],\n",
      "\n",
      "        [[ 0.4283,  0.0359, -0.1070,  ...,  0.2972, -0.9018, -0.3193],\n",
      "         [ 0.2050,  0.1569, -0.2504,  ...,  0.3390, -0.0113, -0.0840],\n",
      "         [ 0.3332, -0.4833, -0.0317,  ...,  0.1579, -0.2697,  0.3430],\n",
      "         ...,\n",
      "         [ 0.0011, -0.1479, -0.0368,  ...,  0.3530, -0.2755,  0.1578],\n",
      "         [-0.1221, -0.1906, -0.1646,  ...,  0.3297, -0.2093,  0.3350],\n",
      "         [-0.0951, -0.1091, -0.0973,  ...,  0.3870, -0.2798,  0.3489]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Length of value is:  36\n",
      "\n",
      "Key\n",
      "tensor([[[ 1.9874e-01,  1.6426e-01,  6.9749e-01,  ...,  4.2358e-01,\n",
      "          -6.2183e-02, -4.6984e-02],\n",
      "         [ 7.2442e-02,  1.1407e-01,  3.5287e-01,  ..., -1.6725e-02,\n",
      "          -1.1122e-01,  4.2619e-01],\n",
      "         [-7.2679e-02,  3.8314e-01,  5.1976e-01,  ...,  3.4564e-01,\n",
      "           3.2858e-01, -1.9727e-01],\n",
      "         ...,\n",
      "         [-9.0593e-02,  2.6615e-01,  3.9025e-01,  ...,  2.4583e-01,\n",
      "           2.2928e-01, -3.9324e-01],\n",
      "         [-3.7560e-01, -2.7803e-01,  1.2735e-02,  ..., -1.2500e-01,\n",
      "           2.5601e-01, -5.3889e-02],\n",
      "         [-2.2078e-02,  5.6972e-04,  2.6594e-01,  ...,  1.9894e-01,\n",
      "           4.0786e-01, -2.2497e-01]],\n",
      "\n",
      "        [[-1.6698e-03,  2.4453e-01,  6.1492e-01,  ...,  5.1140e-01,\n",
      "           4.0055e-02, -3.0985e-01],\n",
      "         [ 5.5714e-02,  1.0663e-01,  3.4048e-01,  ...,  4.0027e-01,\n",
      "           8.8202e-02,  1.5173e-01],\n",
      "         [-2.5809e-01, -6.4474e-02,  2.1802e-01,  ..., -2.2967e-01,\n",
      "           5.2882e-01, -4.2564e-02],\n",
      "         ...,\n",
      "         [-1.1987e-01, -2.6539e-02,  2.0763e-01,  ...,  4.8061e-02,\n",
      "           1.8347e-01, -1.6163e-01],\n",
      "         [-5.7312e-02,  3.4264e-01,  1.0293e-01,  ..., -5.4700e-03,\n",
      "           1.8921e-01, -3.0690e-01],\n",
      "         [-1.9749e-01,  4.7183e-02,  2.1321e-01,  ...,  1.6312e-01,\n",
      "           1.7074e-01, -3.4882e-01]],\n",
      "\n",
      "        [[ 2.6127e-02, -2.6378e-02,  5.4737e-01,  ...,  3.8612e-01,\n",
      "          -2.6172e-02,  1.1641e-02],\n",
      "         [ 1.9133e-01, -3.1609e-01,  5.0434e-01,  ...,  8.5230e-01,\n",
      "           7.0419e-02, -8.8101e-02],\n",
      "         [ 8.8222e-02, -3.3261e-01,  4.6525e-01,  ...,  3.2391e-01,\n",
      "           3.6068e-01, -1.6942e-02],\n",
      "         ...,\n",
      "         [-1.3471e-01,  9.3285e-02, -2.3079e-02,  ..., -1.7994e-02,\n",
      "          -5.9834e-02, -2.5615e-01],\n",
      "         [-1.1655e-01,  5.4843e-02, -5.9558e-02,  ..., -5.0346e-02,\n",
      "           1.8748e-01, -1.4226e-01],\n",
      "         [-1.4365e-01, -1.0492e-02, -7.5086e-02,  ..., -8.9755e-02,\n",
      "           7.8247e-02, -1.4445e-01]]], grad_fn=<ViewBackward0>)\n",
      "length of key is 36\n",
      "Attention Output for each paragraph:\n",
      "tensor([[[ 0.2643, -0.1617,  0.1929,  ...,  0.3843, -0.0266,  0.4792],\n",
      "         [ 0.2602, -0.1484,  0.1933,  ...,  0.3915, -0.0224,  0.4727],\n",
      "         [ 0.2638, -0.1485,  0.1924,  ...,  0.3865, -0.0168,  0.4694],\n",
      "         ...,\n",
      "         [ 0.2624, -0.1445,  0.1847,  ...,  0.3854, -0.0204,  0.4612],\n",
      "         [ 0.2580, -0.1550,  0.1905,  ...,  0.3819, -0.0377,  0.4732],\n",
      "         [ 0.2630, -0.1463,  0.1849,  ...,  0.3832, -0.0232,  0.4654]],\n",
      "\n",
      "        [[ 0.1828, -0.1202,  0.1610,  ...,  0.3802, -0.1412,  0.3229],\n",
      "         [ 0.1788, -0.1274,  0.1625,  ...,  0.3829, -0.1404,  0.3278],\n",
      "         [ 0.1757, -0.1183,  0.1547,  ...,  0.3852, -0.1414,  0.3180],\n",
      "         ...,\n",
      "         [ 0.1780, -0.1180,  0.1541,  ...,  0.3826, -0.1418,  0.3196],\n",
      "         [ 0.1782, -0.1171,  0.1511,  ...,  0.3824, -0.1421,  0.3193],\n",
      "         [ 0.1776, -0.1181,  0.1543,  ...,  0.3824, -0.1429,  0.3198]],\n",
      "\n",
      "        [[ 0.1444, -0.2221, -0.0484,  ...,  0.3771, -0.3036,  0.2080],\n",
      "         [ 0.1454, -0.2174, -0.0454,  ...,  0.3776, -0.3140,  0.2047],\n",
      "         [ 0.1433, -0.2240, -0.0438,  ...,  0.3766, -0.3211,  0.2070],\n",
      "         ...,\n",
      "         [ 0.1467, -0.2075, -0.0509,  ...,  0.3777, -0.3031,  0.1982],\n",
      "         [ 0.1461, -0.2144, -0.0495,  ...,  0.3752, -0.3077,  0.2034],\n",
      "         [ 0.1463, -0.2112, -0.0499,  ...,  0.3762, -0.3057,  0.2007]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Query\")\n",
    "print(Q)\n",
    "print(\"Length og query is: \",len(Q[0]))\n",
    "print(Q[0].size())\n",
    "print()\n",
    "print(\"Value\")\n",
    "print(V)\n",
    "print(\"Length of value is: \",len(V[0]))\n",
    "print()\n",
    "print(\"Key\")\n",
    "print(K)\n",
    "print(\"length of key is\",len(K[0]))\n",
    "print(\"Attention Output for each paragraph:\")\n",
    "print(attention_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 3**: Early Contextual Understanding\n",
    "Self-Attention Mechanism: At this stage, BERT starts capturing inter-token relationships in a sentence. The model can attend to any other token in the sequence, meaning each word can \"look\" at other words in the sentence to understand their relevance.\n",
    "Key Transformation: Each token now has a deeper understanding of its surrounding tokens. For example, \"cat\" in the sentence \"The cat sat on the mat\" attends to \"sat\" and \"mat\" to better understand its role in the sentence.\n",
    "Output: The output of this layer will contain more informative representations where each token’s meaning is enriched with information from the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 4**: Focus on Syntactic Relations\n",
    "Self-Attention Refined: By Layer 4, the model further refines its ability to capture the syntax of the sentence. While previous layers might have captured basic relationships, Layer 4 emphasizes understanding grammatical structures (such as subject-verb-object) and syntactic dependencies.\n",
    "Output: The contextual understanding of tokens is now deeply grounded in their syntactic roles, which is useful for tasks that require understanding sentence structure or parts-of-speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 5**: Enhanced Semantic Understanding\n",
    "Bidirectional Attention: The model continues to learn from both directions (left-to-right and right-to-left) to get a more holistic representation of each token.\n",
    "Abstract Meaning: At this point, Layer 5 starts capturing higher-level meanings and abstract relationships. For example, it learns that \"sat\" in \"The cat sat on the mat\" has a semantic connection to actions and objects in the sentence.\n",
    "Output: The embeddings at this layer are more semantically aware and can distinguish meaning based on context, handling ambiguity better than previous layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 6**: Deepening Contextual Understanding\n",
    "Complex Token Interactions: Layer 6 learns more about how different tokens in the sequence relate to each other. The self-attention mechanism here enables the model to capture more long-range dependencies, meaning the relationship between tokens in different parts of the sentence becomes clearer.\n",
    "Output: The representations are further refined, considering both local context (nearby words) and global context (farther apart words), which makes the model more robust for understanding complex sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 7**: Understanding Disambiguation\n",
    "Polysemy Handling: Layer 7 helps BERT handle polysemy (words with multiple meanings). The model understands that the word \"bank\" in \"river bank\" is different from \"bank\" in \"savings bank.\"\n",
    "Contextualization: BERT’s self-attention mechanism enables it to learn that the meaning of words can change depending on surrounding words. This layer focuses on disambiguating words based on context.\n",
    "Output: Layer 7 ensures that token representations now contain the correct sense of words based on their usage in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 8**: Improving Sentence-Level Understanding\n",
    "Sentence Relationships: At this layer, BERT starts capturing sentence-level relationships, learning how sentences within a paragraph or document connect. For example, it may learn that \"but\" in one sentence negates or contrasts with a statement in a previous sentence.\n",
    "Cross-Sentence Attention: Layer 8’s self-attention mechanism enables it to focus on relationships between distant tokens across sentences, improving its ability to process documents or paragraphs.\n",
    "Output: This representation is now suitable for tasks that require understanding relationships between sentences, such as question-answering or sentence similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 9**: Handling Long-Term Dependencies\n",
    "Long-Term Context: Layer 9 captures even longer-term dependencies between tokens that are far apart in the text. This is especially useful for understanding narrative or discourse-level information, where the relationship between words or phrases can span many sentences.\n",
    "Memory of Context: The model improves its ability to remember important contextual information over long stretches of text, which helps in tasks like document classification or summarization.\n",
    "Output: The output at this layer contains more abstract and deep contextual representations that are important for understanding long passages or complex texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 10**: Contextual Sensitivity\n",
    "Enhanced Sensitivity to Context: This layer further increases the model’s sensitivity to both local and global context. It now has a fine-grained understanding of how the meaning of a word or sentence can change depending on the larger context, including previous and future sentences.\n",
    "Discourse-Level Understanding: Layer 10 starts to focus on discourse-level features, such as coherence between different sections of a text. For instance, understanding how an introductory sentence relates to a conclusion.\n",
    "Output: The embeddings produced by this layer are highly contextual and capable of dealing with the complexities of language involving discourse markers like “however” or “therefore.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 11**: Deep Semantic Representation\n",
    "Abstract Semantic Features: By this stage, BERT is able to capture very abstract semantic features of the text. Layer 11 synthesizes all previous layers' information and begins to create a more holistic understanding of the text, including nuances like sarcasm, irony, or emotion.\n",
    "Task-Specific Focus: Depending on the task at hand (e.g., classification or question-answering), Layer 11 adjusts the representation to emphasize relevant aspects of the text.\n",
    "Output: The output is semantically rich and deeply informed by context, making it suitable for a wide variety of NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 12**: Final Representation\n",
    "Top-Level Understanding: Layer 12 represents the final, fully contextualized representation of each token, considering every preceding layer's transformations. This layer produces the final embeddings for downstream tasks.\n",
    "Rich, Deep Embeddings: This layer’s output is ready to be used for various NLP tasks, including classification, summarization, translation, question-answering, etc.\n",
    "Output: The embeddings here are rich, incorporating all learned information and capable of understanding not only word meanings but also relationships across sentences and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T12:24:22.311457Z",
     "iopub.status.busy": "2024-11-06T12:24:22.310294Z",
     "iopub.status.idle": "2024-11-06T12:24:23.006109Z",
     "shell.execute_reply": "2024-11-06T12:24:23.004741Z",
     "shell.execute_reply.started": "2024-11-06T12:24:22.311405Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 13\n",
      "Layer 4 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[ 7.5136e-02, -3.6864e-01, -1.9043e-01,  ...,  3.6594e-01,\n",
      "           1.9765e-01,  1.4205e-01],\n",
      "         [ 1.0977e+00, -4.3360e-01,  7.1244e-01,  ...,  3.1043e-01,\n",
      "           5.7346e-01, -8.7117e-01],\n",
      "         [-1.1883e+00, -6.1086e-01, -9.5882e-02,  ...,  4.1781e-01,\n",
      "           3.9787e-01,  5.5537e-01],\n",
      "         ...,\n",
      "         [ 1.3817e-01, -4.3395e-01,  3.7636e-01,  ...,  3.2012e-01,\n",
      "          -1.4817e-01, -7.4781e-02],\n",
      "         [-2.8239e-01, -3.2095e-01,  3.7810e-01,  ...,  3.9485e-01,\n",
      "          -2.0048e-01, -2.2469e-01],\n",
      "         [-5.8460e-02, -3.4482e-01,  4.7770e-01,  ...,  3.4661e-01,\n",
      "          -3.8280e-01, -3.2190e-01]],\n",
      "\n",
      "        [[ 5.9574e-04, -3.6950e-01, -1.5953e-01,  ...,  2.3233e-01,\n",
      "           3.0059e-01,  1.5014e-01],\n",
      "         [ 5.1878e-01, -8.9722e-01,  1.1747e+00,  ...,  6.4556e-01,\n",
      "          -1.1184e+00, -8.2523e-01],\n",
      "         [ 1.7718e-01, -1.3090e+00,  6.0979e-01,  ..., -8.8948e-02,\n",
      "          -1.9734e-01,  7.8103e-01],\n",
      "         ...,\n",
      "         [-1.2004e+00,  3.3741e-01, -3.2486e-02,  ...,  2.5291e-01,\n",
      "           3.5771e-01, -8.7503e-01],\n",
      "         [-4.0931e-01, -2.9561e-01,  4.7235e-01,  ..., -2.1719e-01,\n",
      "          -1.5836e-01,  5.9891e-02],\n",
      "         [-6.8964e-02, -1.2528e-01,  1.1641e-01,  ...,  2.1805e-02,\n",
      "           3.7960e-02, -5.9254e-02]],\n",
      "\n",
      "        [[ 2.0613e-03, -3.2343e-01, -1.2649e-01,  ...,  2.1864e-01,\n",
      "           2.3401e-01,  6.3119e-02],\n",
      "         [-1.4176e+00,  8.4340e-01, -1.3087e-01,  ..., -1.7920e-01,\n",
      "          -1.9329e-01,  9.2905e-01],\n",
      "         [-7.5272e-01,  3.7484e-01, -6.4151e-01,  ..., -7.2841e-01,\n",
      "           1.0498e+00, -9.9047e-01],\n",
      "         ...,\n",
      "         [-1.0625e-01, -3.8307e-01,  5.5683e-01,  ...,  7.4289e-02,\n",
      "          -1.8928e-01, -7.7855e-02],\n",
      "         [-1.7435e-01, -4.8460e-01,  4.8858e-01,  ...,  9.0057e-03,\n",
      "          -1.1546e-01, -4.2459e-01],\n",
      "         [-2.5835e-01, -4.1012e-01,  7.3951e-01,  ...,  1.7620e-01,\n",
      "          -2.0597e-01, -4.2069e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 5 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[ 0.3294, -0.7894, -0.5200,  ...,  0.3944,  0.2721,  0.5011],\n",
      "         [ 1.1880, -1.0248,  0.1885,  ...,  0.4302,  0.2172, -0.8390],\n",
      "         [-1.4291, -0.5628, -0.4576,  ...,  0.0975,  0.5580,  1.0511],\n",
      "         ...,\n",
      "         [-0.0349, -0.6928,  0.4435,  ...,  0.0353, -0.0894,  0.0668],\n",
      "         [-0.3459, -0.6975,  0.5115,  ...,  0.0831, -0.1862,  0.0447],\n",
      "         [-0.1555, -0.7052,  0.7526,  ...,  0.1584, -0.3264, -0.0596]],\n",
      "\n",
      "        [[ 0.2599, -0.7477, -0.5595,  ...,  0.2744,  0.1103,  0.5864],\n",
      "         [ 0.4816, -0.9000,  0.8340,  ...,  0.1040, -0.8515, -1.1501],\n",
      "         [ 0.6203, -1.1994, -0.1361,  ..., -0.1231, -0.4633,  0.8819],\n",
      "         ...,\n",
      "         [-0.9089,  0.4360, -0.8011,  ...,  0.9146,  0.5869, -0.3939],\n",
      "         [-0.3178, -0.5358, -0.0627,  ..., -0.2116, -0.2695,  0.2242],\n",
      "         [-0.0235, -0.0604,  0.0083,  ...,  0.0123,  0.0364, -0.0323]],\n",
      "\n",
      "        [[ 0.2266, -0.5062, -0.5250,  ...,  0.1650,  0.2367,  0.3930],\n",
      "         [-1.3594,  0.6578, -0.2906,  ..., -0.0415, -0.7611,  0.5871],\n",
      "         [-0.8370,  0.3101, -0.6379,  ..., -0.1357,  1.1645, -0.7067],\n",
      "         ...,\n",
      "         [-0.2516, -0.9214,  0.7273,  ...,  0.1267,  0.1156,  0.1663],\n",
      "         [-0.2823, -0.9791,  0.6871,  ..., -0.3411,  0.0422, -0.2632],\n",
      "         [-0.3331, -0.8561,  0.8895,  ..., -0.1492, -0.2421, -0.3020]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 6 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[ 0.1428, -0.9996, -0.4865,  ...,  0.1670,  0.2003,  0.6419],\n",
      "         [ 1.3456, -1.3252, -0.0416,  ...,  0.3395,  0.3075, -0.8395],\n",
      "         [-0.9296, -0.4679, -0.6465,  ...,  0.2494,  0.3699,  1.5340],\n",
      "         ...,\n",
      "         [ 0.1310, -0.6485, -0.1893,  ..., -0.2564,  0.1255,  0.2362],\n",
      "         [-0.1341, -0.5259, -0.1372,  ..., -0.2049, -0.1575,  0.3129],\n",
      "         [-0.0449, -0.5993,  0.0724,  ..., -0.1536, -0.3009, -0.0015]],\n",
      "\n",
      "        [[ 0.1693, -0.9800, -0.3560,  ..., -0.1467, -0.1405,  0.6457],\n",
      "         [ 0.8608, -1.1114,  0.6178,  ...,  0.2784, -1.0317, -0.7007],\n",
      "         [ 1.3934, -0.7559, -0.4058,  ...,  0.2381, -0.4141,  0.9650],\n",
      "         ...,\n",
      "         [-0.4688,  0.5044, -1.1193,  ...,  0.6180,  0.4826, -0.5782],\n",
      "         [-0.1178, -0.5016, -0.1813,  ..., -0.3186, -0.3668,  0.6405],\n",
      "         [-0.0131, -0.0340,  0.0323,  ...,  0.0021,  0.0061, -0.0375]],\n",
      "\n",
      "        [[ 0.0140, -0.7370, -0.3650,  ..., -0.0716, -0.0020,  0.3658],\n",
      "         [-1.3341,  0.6945, -0.3277,  ..., -0.2600, -0.1614,  0.2875],\n",
      "         [-0.8810,  0.3159, -0.8543,  ..., -0.2691,  1.0863, -0.5771],\n",
      "         ...,\n",
      "         [-0.2449, -0.5879,  0.1204,  ...,  0.0506,  0.3216,  0.0795],\n",
      "         [-0.3102, -0.7819,  0.1948,  ..., -0.3070,  0.0432, -0.2581],\n",
      "         [-0.2849, -0.7854,  0.3331,  ..., -0.1600, -0.1282, -0.3488]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 7 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[ 5.3462e-02, -1.3014e+00, -4.7477e-01,  ...,  5.1607e-03,\n",
      "           4.8359e-01,  3.8794e-01],\n",
      "         [ 1.2605e+00, -1.3582e+00, -8.8958e-02,  ...,  5.4826e-01,\n",
      "           5.4814e-02, -9.7551e-01],\n",
      "         [-6.4921e-01, -7.3396e-01, -1.0572e+00,  ...,  1.6189e-01,\n",
      "          -2.1067e-02,  1.8826e+00],\n",
      "         ...,\n",
      "         [ 2.6234e-01, -6.9480e-01, -5.3334e-02,  ...,  3.4688e-02,\n",
      "           2.3528e-01,  2.5680e-01],\n",
      "         [-2.7689e-01, -6.4684e-01, -1.2252e-01,  ...,  1.4840e-01,\n",
      "          -2.4912e-01,  3.9093e-01],\n",
      "         [-5.4503e-02, -7.2334e-01,  2.6577e-01,  ...,  3.8303e-01,\n",
      "          -1.8261e-01,  2.7010e-01]],\n",
      "\n",
      "        [[ 1.6799e-01, -9.5737e-01, -5.7550e-01,  ..., -1.8449e-01,\n",
      "          -3.4114e-02,  6.0154e-01],\n",
      "         [ 8.5319e-01, -1.0612e+00,  3.8879e-01,  ...,  1.2966e-01,\n",
      "          -8.1987e-01, -1.0884e+00],\n",
      "         [ 1.2966e+00, -9.6812e-01, -2.6513e-01,  ..., -2.0636e-02,\n",
      "          -3.5815e-01,  9.7274e-01],\n",
      "         ...,\n",
      "         [-4.4005e-01,  4.9131e-01, -5.2959e-01,  ...,  5.8125e-01,\n",
      "           3.4557e-01, -3.7183e-01],\n",
      "         [-5.8995e-01,  8.2638e-03, -4.1436e-01,  ..., -5.9722e-01,\n",
      "          -2.5658e-01,  7.8444e-01],\n",
      "         [ 1.9272e-02, -3.9837e-02, -1.0581e-02,  ..., -1.4576e-02,\n",
      "          -1.5577e-02, -3.7578e-02]],\n",
      "\n",
      "        [[-7.3878e-02, -1.0651e+00, -3.8988e-01,  ..., -1.3520e-01,\n",
      "           1.6382e-01,  3.2614e-01],\n",
      "         [-1.1312e+00,  8.8821e-01, -7.2040e-01,  ..., -5.6990e-01,\n",
      "          -9.5569e-02, -3.2286e-02],\n",
      "         [-9.3020e-01,  7.3309e-01, -4.4124e-01,  ..., -3.1496e-01,\n",
      "           7.2022e-01, -6.4129e-01],\n",
      "         ...,\n",
      "         [-1.8377e-01, -3.3702e-01,  8.8148e-02,  ...,  3.1682e-01,\n",
      "           1.9991e-01,  1.2528e-01],\n",
      "         [-4.3177e-01, -9.3604e-01,  4.2777e-01,  ..., -1.3986e-01,\n",
      "           8.2061e-04, -1.5292e-01],\n",
      "         [-4.8926e-01, -9.8948e-01,  5.8857e-01,  ...,  8.8536e-02,\n",
      "          -1.8015e-01, -2.0072e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 8 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[-0.1967, -1.1540, -0.9466,  ...,  0.1782,  0.3770,  0.2457],\n",
      "         [ 1.1451, -1.2155,  0.4134,  ...,  0.2982,  0.0954, -1.1126],\n",
      "         [-0.8168, -0.7055, -0.9171,  ..., -0.1725, -0.3598,  2.1511],\n",
      "         ...,\n",
      "         [ 0.1630, -0.4364, -0.0256,  ...,  0.1755,  0.0066,  0.4277],\n",
      "         [-0.2966, -0.5611, -0.2775,  ...,  0.2169, -0.2128,  0.7661],\n",
      "         [-0.1491, -0.5510,  0.5040,  ...,  0.5010, -0.3253,  0.5715]],\n",
      "\n",
      "        [[-0.0132, -1.2690, -1.1375,  ..., -0.1603,  0.0340,  0.5389],\n",
      "         [ 0.4755, -1.0566,  0.3525,  ...,  0.3468, -0.6811, -1.2095],\n",
      "         [ 1.0901, -0.7739, -0.0674,  ...,  0.3361, -0.2983,  0.8142],\n",
      "         ...,\n",
      "         [-0.3546,  0.2820, -0.6220,  ...,  0.3596,  0.2831, -0.1938],\n",
      "         [-0.6335, -0.8099, -1.1655,  ..., -0.7605, -0.0108,  0.8532],\n",
      "         [ 0.0075, -0.0225, -0.0219,  ..., -0.0239,  0.0027, -0.0326]],\n",
      "\n",
      "        [[ 0.0684, -1.0884, -0.4567,  ..., -0.2876,  0.6290,  0.2755],\n",
      "         [-1.6789,  0.8225, -0.4994,  ..., -1.0857, -0.2111, -0.2102],\n",
      "         [-1.4741,  0.6114, -0.3572,  ..., -0.3473,  0.0122, -0.6913],\n",
      "         ...,\n",
      "         [-0.2260, -0.0998,  0.0971,  ..., -0.1338, -0.0091,  0.3499],\n",
      "         [-0.1845, -0.5696,  0.4394,  ..., -0.1547, -0.1137,  0.2110],\n",
      "         [-0.1693, -0.6970,  0.4699,  ..., -0.0672, -0.2113,  0.0893]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 9 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[-9.2813e-02, -9.3234e-01, -6.7271e-01,  ...,  3.2440e-01,\n",
      "           3.4278e-01,  2.2675e-01],\n",
      "         [ 8.7127e-01, -7.4360e-01,  3.2220e-01,  ...,  3.9864e-02,\n",
      "          -4.3133e-02, -9.6823e-01],\n",
      "         [-5.0667e-01, -3.4702e-01, -8.3807e-01,  ..., -2.3120e-01,\n",
      "          -1.9918e-01,  1.4797e+00],\n",
      "         ...,\n",
      "         [ 1.5191e-01, -1.0520e-01,  1.7066e-01,  ...,  2.4109e-01,\n",
      "           1.0824e-01,  2.0663e-01],\n",
      "         [-3.8992e-01, -2.5495e-01, -3.2849e-01,  ...,  3.6484e-01,\n",
      "          -7.9079e-02,  5.1197e-01],\n",
      "         [-1.2861e-01, -3.0457e-01,  5.4790e-01,  ...,  7.8063e-01,\n",
      "          -1.4966e-01,  2.8726e-01]],\n",
      "\n",
      "        [[ 1.2254e-01, -1.0129e+00, -1.0064e+00,  ..., -2.1917e-01,\n",
      "          -1.2250e-02,  5.5651e-01],\n",
      "         [ 3.9145e-01, -9.0560e-01,  1.3316e-03,  ...,  5.0207e-01,\n",
      "          -3.9026e-01, -8.3403e-01],\n",
      "         [ 6.7300e-01, -5.0275e-01, -3.5527e-01,  ...,  4.7440e-01,\n",
      "          -3.9690e-01,  9.0802e-01],\n",
      "         ...,\n",
      "         [-3.0224e-01,  2.1801e-01, -6.6729e-01,  ...,  4.9248e-01,\n",
      "           1.9361e-01, -3.0467e-02],\n",
      "         [-9.5545e-01, -4.8480e-01, -1.2954e+00,  ..., -7.4987e-01,\n",
      "           6.6552e-02,  4.9798e-01],\n",
      "         [ 2.2874e-02, -2.8141e-03,  2.1812e-02,  ..., -2.5154e-02,\n",
      "          -3.4136e-02, -6.0706e-02]],\n",
      "\n",
      "        [[ 2.4508e-01, -9.3246e-01, -4.3622e-01,  ..., -1.9687e-01,\n",
      "           3.3780e-01,  3.1903e-01],\n",
      "         [-1.5905e+00,  1.2432e+00, -3.4178e-01,  ..., -8.7217e-01,\n",
      "          -2.5676e-01, -7.0803e-02],\n",
      "         [-1.2643e+00,  2.1878e-01, -2.2558e-01,  ..., -3.5059e-01,\n",
      "          -1.2710e-01, -3.7188e-01],\n",
      "         ...,\n",
      "         [-2.1483e-01,  4.3799e-01,  9.1251e-02,  ..., -3.1351e-01,\n",
      "           2.3380e-02,  5.2806e-01],\n",
      "         [-3.9989e-02, -3.4263e-01,  5.0387e-01,  ..., -2.2035e-01,\n",
      "          -3.5557e-01,  5.9541e-02],\n",
      "         [-1.2385e-01, -5.4461e-01,  5.5646e-01,  ..., -1.8546e-01,\n",
      "          -4.0909e-01, -1.1882e-02]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 10 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[ 0.0366, -0.6225, -0.3671,  ...,  0.3782,  0.5098,  0.3640],\n",
      "         [ 0.5599, -0.6099,  0.3062,  ...,  0.0239,  0.1037, -0.7908],\n",
      "         [-0.6179, -0.1294, -0.6314,  ..., -0.3005, -0.2447,  1.4526],\n",
      "         ...,\n",
      "         [-0.0155, -0.0494,  0.1426,  ...,  0.1620,  0.1893,  0.4069],\n",
      "         [-0.8209, -0.1243, -0.3748,  ...,  0.4786,  0.0635,  0.7785],\n",
      "         [-0.5079, -0.2150,  0.3296,  ...,  0.8614,  0.1598,  0.6555]],\n",
      "\n",
      "        [[ 0.0041, -0.7158, -0.8965,  ...,  0.0694,  0.0449,  0.8719],\n",
      "         [ 0.2959, -0.6691,  0.1124,  ...,  0.4511, -0.4981, -0.3800],\n",
      "         [ 0.2803, -0.5049, -0.0138,  ...,  0.3428, -0.5636,  0.8795],\n",
      "         ...,\n",
      "         [-0.5791,  0.2686, -0.8654,  ...,  0.4943,  0.0607,  0.2705],\n",
      "         [-0.8298, -0.0977, -0.8497,  ..., -0.8771, -0.0146,  0.3907],\n",
      "         [-0.0235, -0.0789,  0.0478,  ..., -0.0052, -0.0121,  0.0035]],\n",
      "\n",
      "        [[ 0.0063, -0.6315, -0.4049,  ...,  0.1158,  0.2148,  0.5876],\n",
      "         [-1.5590,  1.4891, -0.4226,  ..., -1.0490, -0.1567,  0.3008],\n",
      "         [-1.0411,  0.3872, -0.3346,  ..., -0.2544,  0.0551,  0.0106],\n",
      "         ...,\n",
      "         [-0.4102,  1.0689, -0.0855,  ..., -0.5473, -0.3930,  0.5421],\n",
      "         [-0.1130,  0.0580,  0.6220,  ..., -0.3995, -0.4952,  0.2573],\n",
      "         [-0.2922, -0.2942,  0.8275,  ..., -0.2816, -0.4960,  0.2013]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 11 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[-0.4243, -0.7296,  0.0469,  ...,  0.3576,  0.0721,  0.5921],\n",
      "         [ 0.1623, -0.8464,  0.4478,  ..., -0.1631,  0.0755, -0.7213],\n",
      "         [-1.1155, -0.5148, -0.3120,  ..., -0.1590, -0.1574,  1.2838],\n",
      "         ...,\n",
      "         [-0.3501, -0.2126,  0.5314,  ...,  0.0030, -0.4094,  0.2293],\n",
      "         [-1.1018, -0.0892, -0.0526,  ...,  0.5286, -0.1278,  0.6868],\n",
      "         [-0.8745, -0.1772,  0.4164,  ...,  0.6775,  0.0282,  0.7174]],\n",
      "\n",
      "        [[-0.3657, -0.7739, -0.5318,  ..., -0.1826, -0.2263,  1.1186],\n",
      "         [ 0.3047, -0.8362,  0.1725,  ...,  0.1838, -0.8931, -0.5348],\n",
      "         [ 0.2781, -0.5648,  0.4108,  ...,  0.3601, -1.0118,  0.8644],\n",
      "         ...,\n",
      "         [-0.6512,  0.1215, -0.4057,  ...,  0.0704, -0.5599,  0.0046],\n",
      "         [-0.0573, -0.0362, -0.0435,  ..., -0.1411,  0.0170,  0.0349],\n",
      "         [-0.3153, -0.3113,  0.2569,  ...,  0.0617, -0.1563,  0.1021]],\n",
      "\n",
      "        [[-0.5741, -0.6899, -0.1768,  ...,  0.1318, -0.1249,  0.7886],\n",
      "         [-1.9661,  1.1456, -0.5930,  ..., -1.0670, -0.7619,  0.3366],\n",
      "         [-1.5112,  0.1217, -0.5150,  ..., -0.2724, -0.6287,  0.0021],\n",
      "         ...,\n",
      "         [-0.6452,  1.0863, -0.4140,  ..., -0.5344, -0.5781,  0.6377],\n",
      "         [-0.3015,  0.1661,  0.5426,  ..., -0.3384, -0.8765,  0.0588],\n",
      "         [-0.5923, -0.1895,  0.7513,  ..., -0.2034, -0.8562,  0.0688]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 12 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[-0.5999, -0.7031,  0.2343,  ...,  0.1011, -0.0835,  0.3019],\n",
      "         [ 0.3980, -0.6707,  0.3892,  ...,  0.1549, -0.0093, -0.7999],\n",
      "         [-1.2136, -0.6380, -0.0603,  ..., -0.3434, -0.3876,  1.2629],\n",
      "         ...,\n",
      "         [-0.2088, -0.7715,  0.6335,  ..., -0.0303, -0.6510, -0.1413],\n",
      "         [-0.9114, -0.3931,  0.2823,  ...,  0.4546, -0.4330,  0.2143],\n",
      "         [-0.7267, -0.5536,  0.6229,  ...,  0.3378, -0.3974,  0.2652]],\n",
      "\n",
      "        [[-0.3519, -0.6662, -0.4082,  ...,  0.0192, -0.8668,  0.6808],\n",
      "         [ 0.4490, -0.8189,  0.4783,  ...,  0.2493, -0.7866, -0.1197],\n",
      "         [ 0.3785, -0.8105,  0.2290,  ...,  0.0790, -1.0889,  1.1640],\n",
      "         ...,\n",
      "         [-0.4254, -0.1975, -0.5341,  ..., -0.1680, -0.6932,  0.3935],\n",
      "         [ 0.0298,  0.0221, -0.0309,  ...,  0.0184, -0.0172, -0.0130],\n",
      "         [-0.3399, -0.4728,  0.0965,  ...,  0.1993, -0.4377,  0.1519]],\n",
      "\n",
      "        [[-0.5419, -0.4890, -0.1849,  ...,  0.0887, -0.1833,  0.6808],\n",
      "         [-1.4022,  1.0521, -1.0163,  ..., -1.0368, -0.8393,  0.5383],\n",
      "         [-1.1535,  0.1615, -0.7170,  ..., -0.4629, -0.6735,  0.6353],\n",
      "         ...,\n",
      "         [-0.5931,  1.2788, -0.4768,  ..., -0.8664, -0.5139,  0.6053],\n",
      "         [-0.3684,  0.0695,  0.6062,  ..., -0.3731, -1.1353, -0.0240],\n",
      "         [-0.7281, -0.3157,  0.6884,  ..., -0.0807, -1.0407,  0.1677]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n",
      "Layer 13 output shape: torch.Size([3, 29, 768])\n",
      "tensor([[[-0.5101, -0.4079, -0.0336,  ..., -0.0865, -0.2427,  0.6190],\n",
      "         [ 0.2756, -0.3411,  0.4488,  ...,  0.1067,  0.1988, -0.0376],\n",
      "         [-0.6670, -0.3976,  0.0356,  ..., -0.1292, -0.6039,  0.6447],\n",
      "         ...,\n",
      "         [-0.1086, -0.3605,  0.5393,  ..., -0.1725, -0.1922, -0.0163],\n",
      "         [-0.4158, -0.2631,  0.4189,  ...,  0.0064, -0.0752,  0.1376],\n",
      "         [-0.3292, -0.2736,  0.5484,  ..., -0.0975, -0.0985,  0.1490]],\n",
      "\n",
      "        [[-0.1867, -0.4619, -0.4450,  ..., -0.2120, -0.9929,  0.7385],\n",
      "         [ 0.3635, -0.4863,  0.1605,  ...,  0.6052, -0.3208,  0.2520],\n",
      "         [ 0.2859, -0.7466,  0.1477,  ...,  0.3120, -0.9647,  0.7198],\n",
      "         ...,\n",
      "         [-0.4474,  0.0059, -0.4044,  ..., -0.3039, -0.8956,  0.2883],\n",
      "         [ 0.6448, -0.0046, -0.4897,  ...,  0.0649, -0.6467, -0.2352],\n",
      "         [-0.2194, -0.3595,  0.3128,  ...,  0.5487, -0.5778, -0.2170]],\n",
      "\n",
      "        [[-0.3338, -0.4107, -0.1067,  ..., -0.0107, -0.3885,  0.9632],\n",
      "         [-0.4688,  0.5762, -0.7454,  ..., -0.1266, -0.2674,  0.3396],\n",
      "         [-0.7527,  0.2262, -0.6640,  ..., -0.2123, -0.6443,  0.2964],\n",
      "         ...,\n",
      "         [-0.0936,  0.4997, -0.1569,  ..., -0.3789, -0.3976,  0.3569],\n",
      "         [-0.2025,  0.0639,  0.3980,  ..., -0.2051, -0.5811,  0.0950],\n",
      "         [-0.3607, -0.0946,  0.3199,  ..., -0.1105, -0.4772,  0.1739]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "3\n",
      "torch.Size([3, 29, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "paragraphs = [\n",
    "    \"BERT is a transformer model developed by Google. It has revolutionized NLP by introducing bidirectional context.\",\n",
    "    \"Padding is used to ensure that all inputs in a batch have the same length. This simplifies batch processing in deep learning.\",\n",
    "    \"When dealing with paragraphs, BERT can only process up to 512 tokens at a time.\"\n",
    "]\n",
    "\n",
    "# Tokenize the paragraphs and get model inputs\n",
    "inputs = tokenizer(paragraphs, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Get the model outputs (including hidden states for each layer)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract the hidden states for each layer (including the last hidden state)\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "# Print the number of layers in BERT\n",
    "num_layers = len(hidden_states) \n",
    "print(f\"Total number of layers: {num_layers}\")\n",
    "\n",
    "# Access hidden states for Layers 3 to 12\n",
    "for layer_idx in range(3, 13):  \n",
    "    layer_output = hidden_states[layer_idx]\n",
    "    print(f\"Layer {layer_idx + 1} output shape: {layer_output.shape}\")\n",
    "    \n",
    "    print(layer_output)\n",
    "    print(len(layer_output))\n",
    "    print(layer_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
